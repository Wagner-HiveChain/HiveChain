{
  "default_provider": "openai",
  "providers": {
    "openai": {
      "api_key_env": "OPENAI_API_KEY",
      "models": {
        "o1": {
          "engine": "o1",
          "type": "openai",
          "default_temperature": 0.7,
          "max_token_input": 200000,
          "max_token_output": 200000,
          "per_token": {
            "input": 0.015,
            "cached_input": 0.0075,
            "output": 0.060
          }
        },
        "o3-mini": {
          "engine": "o3-mini",
          "type": "openai",
          "default_temperature": 0.7,
          "max_token_input": 200000,
          "max_token_output": 200000,
          "per_token": {
            "input": 0.0011,
            "cached_input": 0.00055,
            "output": 0.0044
          }
        },
        "gpt-4o": {
          "engine": "gpt-4o",
          "type": "openai",
          "default_temperature": 0.7,
          "max_token_input": 128000,
          "max_token_output": 128000,
          "per_token": {
            "input": 0.0025,
            "cached_input": 0.00125,
            "output": 0.0100
          }
        },
        "gpt-4o-mini": {
          "engine": "gpt-4o-mini",
          "type": "openai",
          "default_temperature": 0.7,
          "max_token_input": 128000,
          "max_token_output": 128000,
          "per_token": {
            "input": 0.00015,
            "cached_input": 0.000075,
            "output": 0.0006
          }
        },
        "gpt-3.5-turbo": {
          "engine": "gpt-3.5-turbo",
          "type": "openai",
          "default_temperature": 0.7,
          "max_token_input": 4096,
          "max_token_output": 4096,
          "per_token": {
            "input": 0.0015,
            "cached_input": 0.00075,
            "output": 0.0020
          }
        }
      }
    },
    "deepseek": {
      "api_key_env": "DEEPSEEK_API_KEY",
      "models": {
        "deepseek-v3": {
          "engine": "deepseek-v3",
          "type": "deepseek",
          "default_temperature": 0.6,
          "max_token_input": 128000,
          "max_token_output": 128000,
          "per_token": {
            "input": 0.00014,
            "cached_input": 0.00007,
            "output": 0.00219
          }
        },
        "deepseek-r1": {
          "engine": "deepseek-r1",
          "type": "deepseek",
          "default_temperature": 0.6,
          "max_token_input": 64000,
          "max_token_output": 8000,
          "per_token": {
            "input": 0.00027,
            "cached_input": 0.00007,
            "output": 0.00110
          }
        }
      }
    },
    "anthropic": {
      "api_key_env": "ANTHROPIC_API_KEY",
      "models": {
        "claude-3.5-sonnet": {
          "engine": "claude-3.5-sonnet",
          "type": "anthropic",
          "default_temperature": 0.7,
          "max_token_input": 200000,
          "max_token_output": 200000,
          "per_token": {
            "input": 0.0030,
            "cached_input": 0.0003,
            "output": 0.015
          }
        },
        "claude-3.5-haiku": {
          "engine": "claude-3.5-haiku",
          "type": "anthropic",
          "default_temperature": 0.7,
          "max_token_input": 200000,
          "max_token_output": 200000,
          "per_token": {
            "input": 0.0008,
            "cached_input": 0.00008,
            "output": 0.004
          }
        },
        "claude-3-opus": {
          "engine": "claude-3-opus",
          "type": "anthropic",
          "default_temperature": 0.7,
          "max_token_input": 200000,
          "max_token_output": 200000,
          "per_token": {
            "input": 0.00375,
            "cached_input": 0.0003,
            "output": 0.015
          }
        }
      }
    },
    "google": {
      "api_key_env": "GOOGLE_API_KEY",
      "models": {
        "gemini-2.0-flash": {
          "engine": "gemini-2.0-flash",
          "type": "google",
          "default_temperature": 0.7,
          "max_token_input": 128000,
          "max_token_output": 128000,
          "per_token": {
            "input": 0.0025,
            "cached_input": 0.00125,
            "output": 0.01
          }
        }
      }
    },
    "meta": {
      "api_key_env": "none",
      "models": {
        "llama-3.1": {
          "engine": "llama-3.1",
          "type": "meta",
          "default_temperature": 0.7,
          "max_token_input": 128000,
          "max_token_output": 128000,
          "per_token": {
            "input": null,
            "cached_input": null,
            "output": null
          }
        }
      }
    },
    "huggingface": {
      "api_key_env": "none",
      "models": {
        "mistral-7b": {
          "engine": "mistral-7b",
          "type": "huggingface",
          "default_temperature": 0.5,
          "max_token_input": 8192,
          "max_token_output": 8192,
          "per_token": {
            "input": null,
            "cached_input": null,
            "output": null
          }
        }
      }
    },
    "alibaba": {
      "api_key_env": "none",
      "models": {
        "qwen-2.5-max": {
          "engine": "qwen-2.5-max",
          "type": "alibaba",
          "default_temperature": 0.7,
          "max_token_input": 8192,
          "max_token_output": 8192,
          "per_token": {
            "input": null,
            "cached_input": null,
            "output": null
          }
        }
      }
    }
  },
  "parameter_limits": {
    "temperature": {
      "min": 0.0,
      "max": 1.0
    },
    "max_tokens": {
      "min": 1,
      "max": 200000
    }
  },
  "features": {
    "use_memory": true,
    "use_retrieval": false
  }
}
